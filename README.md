ğŸŒŸ Overview
This project builds a machineâ€‘learning model to predict whether it will rain tomorrow in Australia using the WeatherAUS dataset. It demonstrates a complete endâ€‘toâ€‘end ML workflow:

Data loading and cleaning

Exploratory data analysis (EDA)

Feature engineering

Scaling and oneâ€‘hot encoding

Logistic Regression model training

Evaluation on train/validation/test splits

Saving processed datasets

Predicting on new user inputs

The notebook is ideal for anyone learning practical ML pipelines using Python and scikitâ€‘learn.

ğŸ“Š Dataset
Source: Kaggle â€“ Weather Dataset (Rattle Package)  
Rows: ~145k
Target: RainTomorrow (Yes/No)

Key features include:

Temperature (MinTemp, MaxTemp)

Humidity

Pressure

Wind direction & speed

Rainfall

Cloud cover

Location & date

A timeâ€‘based split is used to avoid data leakage:

Train: data before 2015

Validation: 2015

Test: 2016 onward

ğŸ” Exploratory Data Analysis
The notebook includes visualizations such as:

Rainfall distribution

Temperature relationships

Humidity vs RainTomorrow

Location frequency

Yearâ€‘wise data availability

These insights guide feature selection and preprocessing decisions.

ğŸ› ï¸ Preprocessing Pipeline
ğŸ”§ Handling Missing Values
Numerical columns â†’ mean imputation

Categorical columns â†’ handled by OneHotEncoder

ğŸ“ Scaling
Applied MinMaxScaler to all numeric features

Ensures stable gradient updates for Logistic Regression

ğŸ¨ Encoding
Used OneHotEncoder (sparse_output=False)

Generated 100+ encoded features

Combined with scaled numeric features

ğŸ§¹ Final Feature Set
numeric_cols

encoded_cols  
Total: 123 features

ğŸ¤– Model Training
Model used:
LogisticRegression(solver="liblinear")
Training is performed on:
X_train = train_inputs[numeric_cols + encoded_cols]
y_train = train_target
The model learns a weight for each feature and a bias term.

ğŸ“ˆ Model Performance
Dataset	Accuracy
Train	~85.19%
Validation	~85.41%
Test	~84.22%
A normalized confusion matrix is included in the notebook to visualize prediction quality.

ğŸ’¾ Saving Processed Data
All processed datasets are saved as Parquet files for fast loading and preserved data types:

train_inputs.parquet
val_inputs.parquet
test_inputs.parquet
train_target.parquet
val_target.parquet
test_target.parquet
ğŸ”® Predicting New Inputs
The notebook includes a helper function:

predictSingleInputs(singleInputs)
It:

Converts raw input into a DataFrame

Applies imputation, scaling, and encoding

Predicts Yes/No

Returns probability scores

ğŸš€ Future Enhancements
Try treeâ€‘based models (Random Forest, XGBoost, LightGBM)

Hyperparameter tuning

Address class imbalance

Deploy as a web app or API

Add SHAP or feature importance visualizations
